# Workshop-3: PredicciÃ³n del Ãndice de Felicidad con Machine Learning y Streaming ğŸ“Š  
Este proyecto combina anÃ¡lisis de datos, aprendizaje automÃ¡tico y streaming de datos para predecir el Happiness Score de paÃ­ses utilizando datos del World Happiness Report (2015-2019). A travÃ©s de Jupyter Notebooks, scripts de Python, Kafka para streaming y una base de datos, exploramos, procesamos y modelamos datos para entender quÃ© hace felices a las naciones. 

## Objetivo del Proyecto
El objetivo es construir un modelo de regresiÃ³n para predecir el Happiness Score de paÃ­ses basado en caracterÃ­sticas como el PIB per cÃ¡pita, esperanza de vida, apoyo social, libertad y percepciÃ³n de corrupciÃ³n. El flujo de trabajo incluye:

- **AnÃ¡lisis Exploratorio de Datos (EDA):** Explorar y comparar las caracterÃ­sticas de los datasets de 2015 a 2019, ya que las columnas pueden variar entre aÃ±os.
- **ETL (ExtracciÃ³n, TransformaciÃ³n y Carga):** Extraer caracterÃ­sticas relevantes de los cinco archivos CSV, limpiar y unificar los datos.
- **Entrenamiento del Modelo:** Entrenar un modelo de regresiÃ³n con una divisiÃ³n de datos 70% (entrenamiento) y 30% (prueba).
- **Streaming con Kafka:** Transmitir los datos transformados usando un producer y consumirlos con un consumer para realizar predicciones.
- **Predicciones y Almacenamiento:** Usar el modelo entrenado para predecir el Happiness Score en el conjunto de prueba y almacenar las predicciones junto con las caracterÃ­sticas en una base de datos.
- **EvaluaciÃ³n:** Extraer mÃ©tricas de rendimiento (RÂ²) para evaluar el modelo con los datos de prueba.

## ğŸ“‚ Estructura del Repositorio
```

â”œâ”€â”€ Dockerfile                
â”œâ”€â”€ README.md                
â”œâ”€â”€ data/                  
â”‚   â”œâ”€â”€ 2015.csv
â”‚   â”œâ”€â”€ 2016.csv
â”‚   â”œâ”€â”€ 2017.csv
â”‚   â”œâ”€â”€ 2018.csv
â”‚   â”œâ”€â”€ 2019.csv
â”‚   â”œâ”€â”€ clean\_dataset.csv
â”‚   â””â”€â”€ transformed\_data.json
â”œâ”€â”€ database/
â”‚   â””â”€â”€ db\_connection.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ models/
â”‚   â””â”€â”€ catboost\_model.pkl
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 001-EDA.ipynb
â”‚   â”œâ”€â”€ 002-training.ipynb
â”‚   â”œâ”€â”€ 003-evaluation.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ scripts/
â”œâ”€â”€ consumer.py
â”œâ”€â”€ feature\_selection.py
â””â”€â”€ producer.py

````

## ğŸš€ TecnologÃ­as Utilizadas

- ğŸ Python 3.8+
- ğŸ““ Jupyter Notebook
- ğŸ“Š Scikit-learn
- ğŸ“¡ Kafka (para streaming de datos)
- ğŸ—„ï¸ Base de datos (PostgreSQL)
- ğŸ“„ Archivos CSV
- ğŸ³ Docker y Docker Compose

## ğŸ“‹ Requisitos Previos

Para ejecutar el proyecto, asegÃºrate de tener instalado:

- Python 3.8+ y pip
- Docker y Docker Compose (para Kafka y la base de datos)
- Jupyter Notebook
- Git
- Kafka (incluido en docker-compose.yml)

## ğŸ› ï¸ InstalaciÃ³n

Sigue estos pasos para configurar el proyecto:

**Clona el repositorio:**
```bash
git clone https://github.com/Michelburgos/Workshop-3.git
cd Workshop-3
````

**Crea un entorno virtual :**

```bash
python -m venv venv
source venv/bin/activate 
```

**Instala las dependencias:**

```bash
pip install -r requirements.txt
```

**ğŸ“‹ Contenido de requirements.txt:**

```
pandas
numpy
scikit-learn
matplotlib
seaborn
jupyter
catboost
psycopg2-binary
kafka-python
```

**Configura y ejecuta Docker Compose:**

```bash
docker-compose up -d --build
```

Esto inicia Kafka Y Zookeeper. AsegÃºrate de que los puertos (por ejemplo, 9092 para Kafka) estÃ©n libres.

**Instala y configura PostgreSQL**
   - Instala:  
     ```bash
     sudo apt update
     sudo apt install postgresql -y
     ```
   - Crea usuario y base de datos:
     ```sql
     CREATE USER 'tu_usuario' WITH PASSWORD 'tu_contraseÃ±a';
     ALTER USER 'tu_usuario' CREATEDB;
     createdb -U postgres 'la_db_del_workshop'
     ```

**Configura el archivo `.env`**
   ```
   PG_USER=<tu_usuario>
   PG_PASSWORD=<tu_contraseÃ±a>
   PG_HOST=localhost
   PG_PORT=5432
   PG_DATABASE=<la_db_del_workshop>
    ```

## ğŸ¯ CÃ³mo Ejecutar el Proyecto

1. **ğŸ“Š AnÃ¡lisis Exploratorio de Datos (EDA):**

   ```bash
   jupyter notebook notebooks/001-EDA.ipynb
   ```

2. **ğŸ§¹ Preprocesamiento de Datos (ETL):**

   ```bash
   python scripts/feature_selection.py
   ```

3. **ğŸ¤– Entrenamiento del Modelo:**

   ```bash
   jupyter notebook notebooks/002-training.ipynb
   ```

4. **ğŸ“¡ Streaming con Kafka:**

   ```bash
   python scripts/producer.py
   python scripts/consumer.py
   ```

5. **ğŸ“ˆ EvaluaciÃ³n del Modelo:**

   ```bash
   jupyter notebook notebooks/003-evaluation.ipynb
   ```

6. **ğŸ—„ï¸ ConfiguraciÃ³n de la Base de Datos:**
   Configura las credenciales en `database/db_connection.py`.

## ğŸ“Š Datos

El proyecto utiliza cinco archivos CSV del World Happiness Report (2015-2019) en `data/`, con caracterÃ­sticas como:

* ğŸ’° PIB per cÃ¡pita
* ğŸ©º Esperanza de vida
* ğŸ¤ Apoyo social
* ğŸ•Šï¸ Libertad
* âš–ï¸ PercepciÃ³n de corrupciÃ³n
* ğŸ˜Š Happiness Score (variable objetivo)

## ğŸ’¡ Notas Importantes

* Kafka: AsegÃºrate de que Kafka y Zookeeper estÃ©n corriendo (`docker-compose up -d`) antes de ejecutar `producer.py` o `consumer.py`.
* Base de datos: Configura las credenciales en `db_connection.py`. 
* Docker: Verifica que los puertos en `docker-compose.yml` no estÃ©n en uso.
## ğŸ§‘â€ğŸ’» Autor

**Michel Dahiana Burgos Santos**  
Proyecto acadÃ©mico de IngenierÃ­a de Datos e Inteligencia Artificial
